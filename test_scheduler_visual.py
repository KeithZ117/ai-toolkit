"""
å¿«é€ŸéªŒè¯ DecayingCosineAnnealingWarmRestarts è°ƒåº¦å™¨
åŒ…å«å¯è§†åŒ–å’Œè¯¦ç»†çš„æ•°å€¼æ£€æŸ¥
"""
import math
from toolkit.scheduler import DecayingCosineAnnealingWarmRestarts, get_lr_scheduler
import torch
import matplotlib.pyplot as plt
import numpy as np

# åˆ›å»ºä¸€ä¸ªç®€å•çš„æ¨¡å‹ç”¨äºä¼˜åŒ–å™¨
class DummyModel(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.param = torch.nn.Parameter(torch.tensor(0.0))

def create_optimizer(lr):
    """åˆ›å»ºä¸€ä¸ªçœŸå®çš„ä¼˜åŒ–å™¨ç”¨äºæµ‹è¯•"""
    model = DummyModel()
    return torch.optim.SGD(model.parameters(), lr=lr)

def test_scheduler_basic():
    """åŸºç¡€åŠŸèƒ½æµ‹è¯•"""
    print("=" * 60)
    print("æµ‹è¯• 1: åŸºç¡€åŠŸèƒ½éªŒè¯")
    print("=" * 60)
    
    # æµ‹è¯•å‚æ•°
    initial_lr = 1e-4
    T_0 = 100
    T_mult = 2
    eta_min = 1e-7
    restart_decay = 0.8
    total_steps = 700
    
    opt = create_optimizer(initial_lr)
    sch = DecayingCosineAnnealingWarmRestarts(
        opt, T_0=T_0, T_mult=T_mult, eta_min=eta_min, restart_decay=restart_decay
    )
    
    lrs = []
    for i in range(total_steps):
        sch.step()
        lrs.append(opt.param_groups[0]['lr'])
    
    # æ£€æŸ¥ restart ç‚¹
    jumps = [i for i in range(1, len(lrs)) if lrs[i] - lrs[i-1] > 1e-7]
    
    # é¢„æœŸçš„ restart ç‚¹: T_0=100, ç„¶å 200, 400...
    expected_restarts = [T_0]
    period = T_0
    while expected_restarts[-1] + period * T_mult < total_steps:
        period *= T_mult
        expected_restarts.append(expected_restarts[-1] + period)
    
    print(f"åˆå§‹å­¦ä¹ ç‡: {initial_lr:.2e}")
    print(f"T_0: {T_0}, T_mult: {T_mult}")
    print(f"eta_min: {eta_min:.2e}, restart_decay: {restart_decay}")
    print(f"\né¢„æœŸçš„ restart ç‚¹: {expected_restarts}")
    print(f"å®é™…æ£€æµ‹åˆ°çš„ restart ç‚¹: {jumps}")
    
    # æ£€æŸ¥æ¯ä¸ª restart åå­¦ä¹ ç‡æ˜¯å¦è¡°å‡
    print(f"\næ¯ä¸ª restart çš„èµ·å§‹å­¦ä¹ ç‡:")
    restart_lrs = [initial_lr]
    for idx, restart_step in enumerate(jumps):
        if restart_step < len(lrs):
            lr_at_restart = lrs[restart_step]
            restart_lrs.append(lr_at_restart)
            expected_lr = initial_lr * (restart_decay ** (idx + 1))
            print(f"  Restart {idx+1} (step {restart_step}): {lr_at_restart:.2e} (æœŸæœ›: {expected_lr:.2e})")
    
    print(f"\nå‰ 10 ä¸ªå­¦ä¹ ç‡: {[f'{x:.2e}' for x in lrs[:10]]}")
    print(f"æœ€å 10 ä¸ªå­¦ä¹ ç‡: {[f'{x:.2e}' for x in lrs[-10:]]}")
    
    # éªŒè¯è¡°å‡æ¯”ä¾‹
    if len(restart_lrs) > 1:
        actual_decay = restart_lrs[1] / restart_lrs[0]
        print(f"\nå®é™…è¡°å‡æ¯”ä¾‹: {actual_decay:.4f} (æœŸæœ›: {restart_decay})")
        if abs(actual_decay - restart_decay) < 0.01:
            print("âœ“ è¡°å‡æ¯”ä¾‹æ­£ç¡®")
        else:
            print("âœ— è¡°å‡æ¯”ä¾‹ä¸æ­£ç¡®")
    
    return lrs, jumps

def test_scheduler_edge_cases():
    """è¾¹ç•Œæƒ…å†µæµ‹è¯•"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 2: è¾¹ç•Œæƒ…å†µ")
    print("=" * 60)
    
    # æµ‹è¯• restart_decay = 1.0 (ä¸è¡°å‡)
    print("\n2.1: restart_decay=1.0 (ä¸è¡°å‡)")
    opt = create_optimizer(1e-4)
    sch = DecayingCosineAnnealingWarmRestarts(opt, T_0=50, T_mult=1, eta_min=1e-7, restart_decay=1.0)
    lrs = []
    for i in range(200):
        sch.step()
        lrs.append(opt.param_groups[0]['lr'])
    
    jumps = [i for i in range(1, len(lrs)) if lrs[i] - lrs[i-1] > 1e-7]
    print(f"Restart ç‚¹: {jumps[:5]}")
    if len(jumps) >= 2:
        print(f"ç¬¬ä¸€ä¸ª restart çš„ LR: {lrs[jumps[0]]:.2e}")
        print(f"ç¬¬äºŒä¸ª restart çš„ LR: {lrs[jumps[1]]:.2e}")
        if abs(lrs[jumps[0]] - lrs[jumps[1]]) < 1e-9:
            print("âœ“ restart_decay=1.0 æ—¶å­¦ä¹ ç‡ä¸è¡°å‡")
        else:
            print("âœ— restart_decay=1.0 ä½†å­¦ä¹ ç‡ä»åœ¨è¡°å‡")
    
    # æµ‹è¯• T_mult = 1 (å›ºå®šå‘¨æœŸ)
    print("\n2.2: T_mult=1 (å›ºå®šå‘¨æœŸ)")
    opt = create_optimizer(1e-4)
    sch = DecayingCosineAnnealingWarmRestarts(opt, T_0=50, T_mult=1, eta_min=1e-7, restart_decay=0.9)
    lrs = []
    for i in range(200):
        sch.step()
        lrs.append(opt.param_groups[0]['lr'])
    
    jumps = [i for i in range(1, len(lrs)) if lrs[i] - lrs[i-1] > 1e-7]
    print(f"Restart ç‚¹: {jumps}")
    periods = [jumps[i+1] - jumps[i] for i in range(len(jumps)-1)]
    if periods and all(p == 50 for p in periods):
        print("âœ“ T_mult=1 æ—¶å‘¨æœŸä¿æŒå›ºå®š")
    else:
        print(f"âœ— T_mult=1 ä½†å‘¨æœŸä¸å›ºå®š: {periods}")

def visualize_scheduler():
    """å¯è§†åŒ–å­¦ä¹ ç‡æ›²çº¿"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 3: å¯è§†åŒ–")
    print("=" * 60)
    
    configs = [
        {'T_0': 100, 'T_mult': 2, 'restart_decay': 1.0, 'title': 'restart_decay=1.0 (æ— è¡°å‡)'},
        {'T_0': 100, 'T_mult': 2, 'restart_decay': 0.8, 'title': 'restart_decay=0.8'},
        {'T_0': 100, 'T_mult': 2, 'restart_decay': 0.5, 'title': 'restart_decay=0.5 (å¼ºè¡°å‡)'},
        {'T_0': 100, 'T_mult': 1, 'restart_decay': 0.8, 'title': 'T_mult=1 (å›ºå®šå‘¨æœŸ)'},
        {
            'T_0': 100,
            'T_mult': 2,
            'restart_decay': 0.8,
            'warmup_steps': 50,
            'warmup_start_factor': 0.1,
            'title': 'warmup_steps=50 + restart_decay=0.8',
        },
    ]
    
    initial_lr = 1e-4
    eta_min = 1e-7
    total_steps = 700
    
    cols = 2
    rows = math.ceil(len(configs) / cols)
    fig, axes = plt.subplots(rows, cols, figsize=(15, 5 * rows))
    axes = np.atleast_1d(axes).flatten()
    fig.suptitle('DecayingCosineAnnealingWarmRestarts Scheduler éªŒè¯', fontsize=16)

    for idx, config in enumerate(configs):
        ax = axes[idx]

        opt = create_optimizer(initial_lr)
        if config.get('warmup_steps'):
            sch = get_lr_scheduler(
                "decaying_cosine_with_restarts",
                opt,
                T_0=config['T_0'],
                T_mult=config['T_mult'],
                eta_min=eta_min,
                restart_decay=config['restart_decay'],
                warmup_steps=config['warmup_steps'],
                warmup_start_factor=config.get('warmup_start_factor', 0.0),
            )
        else:
            sch = DecayingCosineAnnealingWarmRestarts(
                opt, T_0=config['T_0'], T_mult=config['T_mult'], 
                eta_min=eta_min, restart_decay=config['restart_decay']
            )

        lrs = []
        for i in range(total_steps):
            sch.step()
            lrs.append(opt.param_groups[0]['lr'])
        
        ax.plot(lrs, linewidth=2)
        ax.set_xlabel('Training Steps')
        ax.set_ylabel('Learning Rate')
        ax.set_title(config['title'])
        ax.grid(True, alpha=0.3)
        ax.set_yscale('log')

        warmup_steps = config.get('warmup_steps', 0)
        if warmup_steps:
            ax.axvspan(0, warmup_steps, color='orange', alpha=0.12, label='warmup')

        jumps = []
        for i in range(1, len(lrs)):
            if warmup_steps and i <= warmup_steps:
                continue
            prev_lr = lrs[i - 1]
            curr_lr = lrs[i]
            if prev_lr <= 0:
                continue
            if curr_lr - prev_lr > 1e-6 and curr_lr / prev_lr > 1.5:
                jumps.append(i)
        for jump in jumps:
            ax.axvline(x=jump, color='r', linestyle='--', alpha=0.5, linewidth=1)

    for idx in range(len(configs), len(axes)):
        axes[idx].axis('off')

    plt.tight_layout(rect=[0, 0, 1, 0.96])
    
    # ä¿å­˜å›¾ç‰‡
    output_path = 'scheduler_validation.png'
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print(f"\nâœ“ å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°: {output_path}")
    
    # å°è¯•æ˜¾ç¤ºå›¾è¡¨
    try:
        plt.show()
    except:
        print("(æ— æ³•æ˜¾ç¤ºå›¾è¡¨çª—å£ï¼Œè¯·æŸ¥çœ‹ä¿å­˜çš„å›¾ç‰‡)")

def test_comparison_with_pytorch():
    """ä¸ PyTorch åŸç”Ÿè°ƒåº¦å™¨å¯¹æ¯”"""
    print("\n" + "=" * 60)
    print("æµ‹è¯• 4: ä¸ PyTorch CosineAnnealingWarmRestarts å¯¹æ¯”")
    print("=" * 60)
    
    initial_lr = 1e-4
    T_0 = 100
    T_mult = 2
    eta_min = 1e-7
    total_steps = 300
    
    # PyTorch åŸç”Ÿè°ƒåº¦å™¨
    opt1 = create_optimizer(initial_lr)
    sch1 = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
        opt1, T_0=T_0, T_mult=T_mult, eta_min=eta_min
    )
    lrs_pytorch = []
    for i in range(total_steps):
        sch1.step()
        lrs_pytorch.append(opt1.param_groups[0]['lr'])
    
    # æˆ‘ä»¬çš„è°ƒåº¦å™¨ (restart_decay=1.0 æ—¶åº”è¯¥ä¸€æ ·)
    opt2 = create_optimizer(initial_lr)
    sch2 = DecayingCosineAnnealingWarmRestarts(
        opt2, T_0=T_0, T_mult=T_mult, eta_min=eta_min, restart_decay=1.0
    )
    lrs_ours = []
    for i in range(total_steps):
        sch2.step()
        lrs_ours.append(opt2.param_groups[0]['lr'])
    
    # è®¡ç®—å·®å¼‚
    diff = [abs(a - b) for a, b in zip(lrs_pytorch, lrs_ours)]
    max_diff = max(diff)
    avg_diff = sum(diff) / len(diff)
    
    print(f"æœ€å¤§å·®å¼‚: {max_diff:.2e}")
    print(f"å¹³å‡å·®å¼‚: {avg_diff:.2e}")
    
    if max_diff < 1e-10:
        print("âœ“ restart_decay=1.0 æ—¶ä¸ PyTorch åŸç”Ÿè°ƒåº¦å™¨å®Œå…¨ä¸€è‡´")
    else:
        print("âœ— ä¸ PyTorch åŸç”Ÿè°ƒåº¦å™¨å­˜åœ¨å·®å¼‚")
        print(f"å‰ 10 ä¸ªå·®å¼‚: {[f'{x:.2e}' for x in diff[:10]]}")

def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "ğŸ”" * 30)
    print("DecayingCosineAnnealingWarmRestarts å®Œæ•´éªŒè¯")
    print("ğŸ”" * 30 + "\n")
    
    # è¿è¡Œæµ‹è¯•
    lrs, jumps = test_scheduler_basic()
    test_scheduler_edge_cases()
    test_comparison_with_pytorch()
    
    # å¯è§†åŒ–
    try:
        visualize_scheduler()
    except Exception as e:
        print(f"\nâš  å¯è§†åŒ–å¤±è´¥: {e}")
        print("å¯èƒ½éœ€è¦å®‰è£… matplotlib: pip install matplotlib")
    
    print("\n" + "=" * 60)
    print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ!")
    print("=" * 60)
    
    return lrs, jumps

if __name__ == "__main__":
    lrs, jumps = run_all_tests()
