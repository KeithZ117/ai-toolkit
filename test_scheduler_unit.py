"""
å•å…ƒæµ‹è¯•: DecayingCosineAnnealingWarmRestarts
å¿«é€ŸéªŒè¯æ ¸å¿ƒåŠŸèƒ½æ˜¯å¦æ­£ç¡®
"""
import sys
import math
from toolkit.scheduler import DecayingCosineAnnealingWarmRestarts, get_lr_scheduler
import torch

# åˆ›å»ºä¸€ä¸ªç®€å•çš„æ¨¡å‹ç”¨äºä¼˜åŒ–å™¨
class DummyModel(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.param = torch.nn.Parameter(torch.tensor(0.0))

def create_optimizer(lr):
    """åˆ›å»ºä¸€ä¸ªçœŸå®çš„ä¼˜åŒ–å™¨ç”¨äºæµ‹è¯•"""
    model = DummyModel()
    return torch.optim.SGD(model.parameters(), lr=lr)

def test_basic_initialization():
    """æµ‹è¯•åŸºç¡€åˆå§‹åŒ–"""
    print("æµ‹è¯• 1: åŸºç¡€åˆå§‹åŒ–...", end=" ")
    opt = create_optimizer(1e-4)
    sch = DecayingCosineAnnealingWarmRestarts(opt, T_0=100, T_mult=2, eta_min=1e-7, restart_decay=0.8)
    assert sch.T_0 == 100
    assert sch.T_mult == 2
    assert sch.eta_min == 1e-7
    assert sch.restart_decay == 0.8
    print("âœ“ PASS")
    return True

def test_restart_decay():
    """æµ‹è¯• restart åå­¦ä¹ ç‡æ˜¯å¦æ­£ç¡®è¡°å‡"""
    print("æµ‹è¯• 2: Restart è¡°å‡...", end=" ")
    
    initial_lr = 1e-4
    restart_decay = 0.8
    T_0 = 100
    
    opt = create_optimizer(initial_lr)
    sch = DecayingCosineAnnealingWarmRestarts(opt, T_0=T_0, T_mult=2, eta_min=1e-7, restart_decay=restart_decay)
    
    # è¿è¡Œåˆ°ç¬¬ä¸€ä¸ª restart
    for _ in range(T_0):
        sch.step()
    
    # ä¸‹ä¸€æ­¥åº”è¯¥æ˜¯ restartï¼Œå­¦ä¹ ç‡åº”è¯¥è¡°å‡
    sch.step()
    lr_after_first_restart = opt.param_groups[0]['lr']
    expected_lr = initial_lr * restart_decay
    
    # å…è®¸ 1% çš„æµ®ç‚¹è¯¯å·®
    error = abs(lr_after_first_restart - expected_lr) / expected_lr
    assert error < 0.01, f"å­¦ä¹ ç‡ä¸åŒ¹é…: {lr_after_first_restart:.6e} vs {expected_lr:.6e}"
    
    print("âœ“ PASS")
    return True

def test_period_multiplication():
    """æµ‹è¯•å‘¨æœŸå€å¢"""
    print("æµ‹è¯• 3: å‘¨æœŸå€å¢...", end=" ")
    
    T_0 = 50
    T_mult = 2
    
    opt = create_optimizer(1e-4)
    sch = DecayingCosineAnnealingWarmRestarts(opt, T_0=T_0, T_mult=T_mult, eta_min=1e-7, restart_decay=0.8)
    
    lrs = []
    for _ in range(250):
        sch.step()
        lrs.append(opt.param_groups[0]['lr'])
    
    # æ£€æµ‹ restart ç‚¹
    jumps = [i for i in range(1, len(lrs)) if lrs[i] - lrs[i-1] > 1e-7]
    
    # é¢„æœŸ: 50, 150 (50 + 100), ...
    expected_jumps = [50, 150]
    
    assert jumps[:2] == expected_jumps, f"Restart ç‚¹ä¸æ­£ç¡®: {jumps[:2]} vs {expected_jumps}"
    
    print("âœ“ PASS")
    return True

def test_eta_min_constraint():
    """æµ‹è¯•æœ€å°å­¦ä¹ ç‡çº¦æŸ"""
    print("æµ‹è¯• 4: æœ€å°å­¦ä¹ ç‡çº¦æŸ...", end=" ")
    
    eta_min = 1e-6
    opt = create_optimizer(1e-4)
    sch = DecayingCosineAnnealingWarmRestarts(opt, T_0=100, T_mult=2, eta_min=eta_min, restart_decay=0.5)
    
    lrs = []
    for _ in range(1000):
        sch.step()
        lrs.append(opt.param_groups[0]['lr'])
    
    min_lr = min(lrs)
    
    # æœ€å°å­¦ä¹ ç‡åº”è¯¥ä¸ä½äº eta_min (å…è®¸å¾®å°çš„æµ®ç‚¹è¯¯å·®)
    assert min_lr >= eta_min * 0.99, f"å­¦ä¹ ç‡ä½äº eta_min: {min_lr:.6e} < {eta_min:.6e}"
    
    print("âœ“ PASS")
    return True

def test_no_decay_compatibility():
    """æµ‹è¯• restart_decay=1.0 æ—¶ä¸ PyTorch åŸç”Ÿè°ƒåº¦å™¨ä¸€è‡´"""
    print("æµ‹è¯• 5: PyTorch å…¼å®¹æ€§ (restart_decay=1.0)...", end=" ")
    
    initial_lr = 1e-4
    T_0 = 100
    T_mult = 2
    eta_min = 1e-7
    
    # PyTorch åŸç”Ÿ
    opt1 = create_optimizer(initial_lr)
    sch1 = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
        opt1, T_0=T_0, T_mult=T_mult, eta_min=eta_min
    )
    
    # æˆ‘ä»¬çš„å®ç°
    opt2 = create_optimizer(initial_lr)
    sch2 = DecayingCosineAnnealingWarmRestarts(
        opt2, T_0=T_0, T_mult=T_mult, eta_min=eta_min, restart_decay=1.0
    )
    
    for _ in range(300):
        sch1.step()
        sch2.step()
        
        diff = abs(opt1.param_groups[0]['lr'] - opt2.param_groups[0]['lr'])
        assert diff < 1e-10, f"ä¸ PyTorch å·®å¼‚è¿‡å¤§: {diff:.2e}"
    
    print("âœ“ PASS")
    return True

def test_cosine_shape():
    """æµ‹è¯• cosine æ›²çº¿å½¢çŠ¶"""
    print("æµ‹è¯• 6: Cosine æ›²çº¿å½¢çŠ¶...", end=" ")
    
    T_0 = 100
    opt = create_optimizer(1e-4)
    sch = DecayingCosineAnnealingWarmRestarts(opt, T_0=T_0, T_mult=1, eta_min=1e-7, restart_decay=0.8)
    
    lrs = []
    for _ in range(T_0):
        sch.step()
        lrs.append(opt.param_groups[0]['lr'])
    
    # Cosine æ›²çº¿åº”è¯¥ï¼šèµ·å§‹é«˜ï¼Œä¸­é—´ä¸‹é™ï¼Œç»“æŸæœ€ä½
    assert lrs[0] > lrs[T_0//2], "Cosine æ›²çº¿å½¢çŠ¶ä¸æ­£ç¡®"
    assert lrs[T_0//2] > lrs[-1], "Cosine æ›²çº¿å½¢çŠ¶ä¸æ­£ç¡®"
    
    # å‰åŠéƒ¨åˆ†åº”è¯¥å•è°ƒé€’å‡
    for i in range(1, T_0//2):
        assert lrs[i] <= lrs[i-1], f"å‰åŠéƒ¨åˆ†åº”è¯¥å•è°ƒé€’å‡ï¼Œä½†åœ¨ step {i} å¤„å¢åŠ äº†"
    
    print("âœ“ PASS")
    return True

def test_multiple_restarts():
    """æµ‹è¯•å¤šæ¬¡ restart çš„è¡°å‡"""
    print("æµ‹è¯• 7: å¤šæ¬¡ Restart è¡°å‡...", end=" ")
    
    initial_lr = 1e-4
    restart_decay = 0.75
    T_0 = 50
    
    opt = create_optimizer(initial_lr)
    sch = DecayingCosineAnnealingWarmRestarts(opt, T_0=T_0, T_mult=1, eta_min=1e-7, restart_decay=restart_decay)
    
    lrs = []
    for _ in range(250):
        sch.step()
        lrs.append(opt.param_groups[0]['lr'])
    
    # æ£€æµ‹ restart ç‚¹
    jumps = [i for i in range(1, len(lrs)) if lrs[i] - lrs[i-1] > 1e-7]
    
    # è‡³å°‘åº”è¯¥æœ‰ 3 æ¬¡ restart
    assert len(jumps) >= 3, f"åº”è¯¥è‡³å°‘æœ‰ 3 æ¬¡ restartï¼Œä½†åªæœ‰ {len(jumps)} æ¬¡"
    
    # æ£€æŸ¥æ¯æ¬¡ restart çš„è¡°å‡æ¯”ä¾‹
    restart_lrs = [initial_lr] + [lrs[j] for j in jumps]
    
    for i in range(1, len(restart_lrs)):
        actual_ratio = restart_lrs[i] / restart_lrs[i-1]
        expected_ratio = restart_decay
        error = abs(actual_ratio - expected_ratio) / expected_ratio
        assert error < 0.01, f"ç¬¬ {i} æ¬¡ restart è¡°å‡æ¯”ä¾‹ä¸æ­£ç¡®: {actual_ratio:.4f} vs {expected_ratio:.4f}"
    
    print("âœ“ PASS")
    return True

def test_warmup_integration():
    """æµ‹è¯• warmup + decaying cosine ç»„åˆ"""
    print("æµ‹è¯• 8: Warmup ç»„åˆ...", end=" ")

    initial_lr = 1e-4
    warmup_steps = 8
    warmup_start_factor = 0.1

    opt = create_optimizer(initial_lr)
    sch = get_lr_scheduler(
        "decaying_cosine_with_restarts",
        opt,
        T_0=40,
        T_mult=1,
        eta_min=1e-7,
        restart_decay=0.8,
        warmup_steps=warmup_steps,
        warmup_start_factor=warmup_start_factor,
    )

    warmup_lrs = []
    for _ in range(warmup_steps):
        sch.step()
        warmup_lrs.append(opt.param_groups[0]['lr'])

    assert warmup_lrs[0] < warmup_lrs[-1] <= initial_lr + 1e-12
    assert all(warmup_lrs[i] <= warmup_lrs[i + 1] + 1e-12 for i in range(len(warmup_lrs) - 1))

    sch.step()
    lr_after_warmup = opt.param_groups[0]['lr']
    assert abs(lr_after_warmup - initial_lr) < initial_lr * 0.01

    print("âœ“ PASS")
    return True

def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "=" * 60)
    print("DecayingCosineAnnealingWarmRestarts å•å…ƒæµ‹è¯•")
    print("=" * 60 + "\n")
    
    tests = [
        test_basic_initialization,
        test_restart_decay,
        test_period_multiplication,
        test_eta_min_constraint,
        test_no_decay_compatibility,
        test_cosine_shape,
        test_multiple_restarts,
        test_warmup_integration,
    ]
    
    passed = 0
    failed = 0
    
    for test in tests:
        try:
            if test():
                passed += 1
        except AssertionError as e:
            print(f"âœ— FAIL: {e}")
            failed += 1
        except Exception as e:
            print(f"âœ— ERROR: {e}")
            failed += 1
    
    print("\n" + "=" * 60)
    print(f"æµ‹è¯•ç»“æœ: {passed} é€šè¿‡, {failed} å¤±è´¥")
    print("=" * 60)
    
    if failed == 0:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡! DecayingCosineAnnealingWarmRestarts å·¥ä½œæ­£å¸¸!")
        return 0
    else:
        print(f"\nâš  æœ‰ {failed} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥å®ç°ã€‚")
        return 1

if __name__ == "__main__":
    exit_code = run_all_tests()
    sys.exit(exit_code)
