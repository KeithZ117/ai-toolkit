# DecayingCosineAnnealingWarmRestarts å¿«é€ŸéªŒè¯æŒ‡å—

## ğŸ¯ ç›®çš„
å¿«é€ŸéªŒè¯ä½ å®ç°çš„ `DecayingCosineAnnealingWarmRestarts` å­¦ä¹ ç‡è°ƒåº¦å™¨æ˜¯å¦å·¥ä½œæ­£å¸¸ã€‚

## ğŸ“‹ æœ€å¿«éªŒè¯æ–¹æ³•

### âš¡ æ¨èï¼šä¸€é”®éªŒè¯ï¼ˆé€‚ç”¨äº runpod ç­‰æ‰€æœ‰ç¯å¢ƒï¼‰

```bash
python test_scheduler.py
```

**ç‰¹ç‚¹ï¼š**
- âœ… 30 ç§’å†…å®Œæˆæ‰€æœ‰æ ¸å¿ƒæµ‹è¯•
- âœ… 4 ä¸ªå…³é”®åŠŸèƒ½éªŒè¯
- âœ… æ¸…æ™°çš„ PASS/FAIL è¾“å‡º
- âœ… è‡ªåŠ¨ä¸ PyTorch åŸç”Ÿè°ƒåº¦å™¨å¯¹æ¯”
- âœ… æ— éœ€é¢å¤–ä¾èµ–ï¼ˆä¸éœ€è¦ matplotlibï¼‰

**æµ‹è¯•å†…å®¹ï¼š**
1. åŸºç¡€åŠŸèƒ½ï¼ˆRestart ç‚¹å’Œè¡°å‡æ¯”ä¾‹ï¼‰
2. PyTorch å…¼å®¹æ€§ï¼ˆrestart_decay=1.0ï¼‰
3. ä¸åŒ restart_decay å‚æ•°æµ‹è¯•
4. Cosine æ›²çº¿å½¢çŠ¶éªŒè¯

---

## ğŸ“‹ å…¶ä»–éªŒè¯æ–¹æ³•

```bash
python test_scheduler_simple.py
```

**ç‰¹ç‚¹ï¼š**
- âœ… è¯¦ç»†çš„æ•°å€¼è¾“å‡º
- âœ… æ˜¾ç¤ºæ¯ä¸ª restart ç‚¹çš„å­¦ä¹ ç‡
- âœ… æ£€æŸ¥è¡°å‡æ¯”ä¾‹
- âœ… ä¸ PyTorch åŸç”Ÿè°ƒåº¦å™¨å¯¹æ¯”

**è¾“å‡ºç¤ºä¾‹ï¼š**
```
âœ“ æ£€æŸ¥ 1: Restart ç‚¹ä½ç½®
  é¢„æœŸ restart åœ¨: [100, 300, 700]
  å®é™… restart åœ¨: [100, 300, 700]
  âœ“ PASS - Restart ç‚¹å®Œå…¨æ­£ç¡®!

âœ“ æ£€æŸ¥ 2: æ¯æ¬¡ Restart çš„å­¦ä¹ ç‡è¡°å‡
  Restart #1 (step 100): 8.000000e-05 (æœŸæœ›: 8.000000e-05, è¯¯å·®: 0.00%) âœ“
  Restart #2 (step 300): 6.400000e-05 (æœŸæœ›: 6.400000e-05, è¯¯å·®: 0.00%) âœ“
```

---

### æ–¹æ³• 3: å¯è§†åŒ–éªŒè¯ï¼ˆéœ€è¦ matplotlibï¼‰

```bash
# å…ˆå®‰è£… matplotlibï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
pip install matplotlib

# è¿è¡Œå¯è§†åŒ–è„šæœ¬
python test_scheduler_visual.py
```

**ç‰¹ç‚¹ï¼š**
- âœ… ç”Ÿæˆå­¦ä¹ ç‡æ›²çº¿å›¾
- âœ… å¯¹æ¯”ä¸åŒ restart_decay å‚æ•°
- âœ… å¯è§†åŒ–éªŒè¯ cosine å½¢çŠ¶å’Œ restart ç‚¹
- âœ… ä¿å­˜å›¾ç‰‡åˆ° `scheduler_validation.png`

**ç”Ÿæˆ 5 ä¸ªå¯¹æ¯”å›¾ï¼š**
1. restart_decay=1.0ï¼ˆæ— è¡°å‡ï¼‰
2. restart_decay=0.8ï¼ˆä¸­ç­‰è¡°å‡ï¼‰
3. restart_decay=0.5ï¼ˆå¼ºè¡°å‡ï¼‰
4. T_mult=1ï¼ˆå›ºå®šå‘¨æœŸï¼‰
5. warmup_steps=50 + restart_decay=0.8ï¼ˆå¸¦é¢„çƒ­ï¼‰

---

### æ–¹æ³• 4: åŸå§‹æµ‹è¯•è„šæœ¬

```bash
python tmp_sch.py
```

ä½ åŸæœ‰çš„ç®€å•æµ‹è¯•è„šæœ¬ï¼Œå¿«é€ŸæŸ¥çœ‹åŸºæœ¬è¡Œä¸ºã€‚

---

## ğŸ” å…³é”®éªŒè¯ç‚¹

ä¸€ä¸ªæ­£å¸¸å·¥ä½œçš„ `DecayingCosineAnnealingWarmRestarts` åº”è¯¥æ»¡è¶³ï¼š

### 1. **Restart ç‚¹æ­£ç¡®**
- ç¬¬ä¸€ä¸ª restart åœ¨ step `T_0`
- å¦‚æœ `T_mult=2`ï¼Œåç»­ restart åœ¨ `T_0 + 2*T_0`, `T_0 + 2*T_0 + 4*T_0`, ...
- å¦‚æœ `T_mult=1`ï¼Œæ¯éš” `T_0` å°±æœ‰ä¸€ä¸ª restart

### 2. **è¡°å‡æ­£ç¡®**
- æ¯æ¬¡ restart åï¼Œæœ€å¤§å­¦ä¹ ç‡åº”è¯¥ä¹˜ä»¥ `restart_decay`
- ä¾‹å¦‚ï¼šåˆå§‹ `1e-4`ï¼Œ`restart_decay=0.8`
  - ç¬¬ 1 æ¬¡ restart: `8e-5`
  - ç¬¬ 2 æ¬¡ restart: `6.4e-5`
  - ç¬¬ 3 æ¬¡ restart: `5.12e-5`

### 3. **Cosine å½¢çŠ¶**
- æ¯ä¸ªå‘¨æœŸå†…ï¼Œå­¦ä¹ ç‡åº”è¯¥æŒ‰ cosine æ›²çº¿ä»é«˜åˆ°ä½å¹³æ»‘ä¸‹é™
- ä¸åº”è¯¥æœ‰çªç„¶çš„è·³è·ƒï¼ˆé™¤äº† restart ç‚¹ï¼‰

### 4. **æœ€å°å­¦ä¹ ç‡çº¦æŸ**
- å­¦ä¹ ç‡æ°¸è¿œä¸åº”è¯¥ä½äº `eta_min`

### 5. **ä¸ PyTorch å…¼å®¹**
- å½“ `restart_decay=1.0` æ—¶ï¼Œåº”è¯¥ä¸ `torch.optim.lr_scheduler.CosineAnnealingWarmRestarts` å®Œå…¨ä¸€è‡´

### 6. **Warmupï¼ˆå¯é€‰ï¼‰**
- å¦‚æœè®¾ç½®äº† `warmup_steps`ï¼Œé¢„çƒ­é˜¶æ®µçš„å­¦ä¹ ç‡åº”è¯¥çº¿æ€§å¢é•¿åˆ°åŸºç¡€å­¦ä¹ ç‡
- é¢„çƒ­ç»“æŸåå†è¿›å…¥ cosine é€€ç«å‘¨æœŸï¼Œé‡å¯æ—¶ä¾æ—§éµå¾ª `restart_decay`

---

## ğŸ“Š å¿«é€Ÿè¯Šæ–­

å¦‚æœæµ‹è¯•å¤±è´¥ï¼ŒæŸ¥çœ‹ä»¥ä¸‹å†…å®¹ï¼š

### âŒ Restart ç‚¹ä¸å¯¹
â†’ æ£€æŸ¥ `step()` å‡½æ•°ä¸­çš„å‘¨æœŸè®¡ç®—é€»è¾‘

### âŒ è¡°å‡æ¯”ä¾‹ä¸å¯¹
â†’ æ£€æŸ¥ `_update_base_lrs()` å‡½æ•°ä¸­çš„ `factor` è®¡ç®—

### âŒ å­¦ä¹ ç‡æ›²çº¿ä¸å¹³æ»‘
â†’ æ£€æŸ¥ `get_lr()` å‡½æ•°ä¸­çš„ cosine å…¬å¼

### âŒ æœ€å°å­¦ä¹ ç‡è¢«çªç ´
â†’ æ£€æŸ¥ `eta_min` æ˜¯å¦ä¹Ÿåœ¨è¡°å‡ï¼ˆåº”è¯¥è¡°å‡ï¼‰

---

## ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹

åœ¨è®­ç»ƒä»£ç ä¸­ä½¿ç”¨ï¼š

```python
from toolkit.scheduler import (
    DecayingCosineAnnealingWarmRestarts,
    get_lr_scheduler,
)

# åˆ›å»ºä¼˜åŒ–å™¨
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)

# æ–¹æ¡ˆ Aï¼šçº¯ decaying cosine with restarts
scheduler = DecayingCosineAnnealingWarmRestarts(
    optimizer,
    T_0=1000,           # ç¬¬ä¸€ä¸ªå‘¨æœŸ 1000 æ­¥
    T_mult=2,           # æ¯æ¬¡å‘¨æœŸç¿»å€
    eta_min=1e-7,       # æœ€å°å­¦ä¹ ç‡
    restart_decay=0.8   # æ¯æ¬¡ restart è¡°å‡åˆ° 80%
)

# æ–¹æ¡ˆ Bï¼šåŠ å…¥ warmupï¼Œåªåœ¨å¼€å¤´é¢„çƒ­ä¸€æ¬¡
# scheduler = get_lr_scheduler(
#     "decaying_cosine_with_restarts",
#     optimizer,
#     T_0=1000,
#     T_mult=2,
#     eta_min=1e-7,
#     restart_decay=0.8,
#     warmup_steps=500,
#     warmup_start_factor=0.1,
# )

# è®­ç»ƒå¾ªç¯
for epoch in range(num_epochs):
    for batch in dataloader:
        # ... è®­ç»ƒä»£ç  ...
        optimizer.step()
        scheduler.step()  # æ¯ä¸ª batch åè°ƒç”¨
```

---

## ğŸ‰ æœŸæœ›ç»“æœ

å¦‚æœæ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡ï¼Œä½ åº”è¯¥çœ‹åˆ°ï¼š

```
ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡! DecayingCosineAnnealingWarmRestarts å·¥ä½œæ­£å¸¸!
```

è¿™è¡¨ç¤ºä½ çš„å®ç°ï¼š
- âœ… Restart æœºåˆ¶æ­£å¸¸
- âœ… è¡°å‡è®¡ç®—æ­£ç¡®
- âœ… Cosine æ›²çº¿å½¢çŠ¶æ­£ç¡®
- âœ… ä¸ PyTorch å…¼å®¹
- âœ… å¯ä»¥å®‰å…¨ä½¿ç”¨

---

## ğŸ“ å‚æ•°è¯´æ˜

| å‚æ•° | è¯´æ˜ | å…¸å‹å€¼ |
|------|------|--------|
| `T_0` | ç¬¬ä¸€ä¸ª restart å‘¨æœŸçš„é•¿åº¦ | 100-1000 |
| `T_mult` | å‘¨æœŸå€å¢å› å­ | 1 æˆ– 2 |
| `eta_min` | æœ€å°å­¦ä¹ ç‡ | 1e-7 |
| `restart_decay` | Restart è¡°å‡å› å­ | 0.5-1.0 |
| `warmup_steps` | é¢„çƒ­æ­¥æ•°ï¼ˆå¯é€‰ï¼‰ | 0-1000 |
| `warmup_start_factor` | é¢„çƒ­èµ·å§‹ç³»æ•°ï¼ˆå¯é€‰ï¼‰ | 0.0-0.1 |

**å»ºè®®ï¼š**
- å¯¹äºé•¿è®­ç»ƒï¼š`T_0=1000`, `T_mult=2`, `restart_decay=0.8`
- å¯¹äºçŸ­è®­ç»ƒï¼š`T_0=100`, `T_mult=1`, `restart_decay=0.9`
- ä¸æƒ³è¡°å‡ï¼š`restart_decay=1.0`ï¼ˆç­‰åŒäº PyTorch åŸç”Ÿï¼‰
- éœ€è¦æ›´å¹³æ»‘çš„èµ·æ­¥ï¼šåŠ å…¥ `warmup_steps`ï¼ˆä¾‹å¦‚ 500ï¼‰å’Œ `warmup_start_factor=0.1`

---

## ğŸ› è°ƒè¯•æŠ€å·§

å¦‚æœæƒ³çœ‹å­¦ä¹ ç‡å˜åŒ–ï¼š

```python
scheduler = DecayingCosineAnnealingWarmRestarts(...)
lrs = []
for i in range(500):
    scheduler.step()
    lrs.append(optimizer.param_groups[0]['lr'])

# ç»˜åˆ¶æ›²çº¿
import matplotlib.pyplot as plt
plt.plot(lrs)
plt.yscale('log')
plt.show()
```
